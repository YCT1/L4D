{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Yekta Can Tursun\n",
    "### ID: 150170105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Gradient Descent and Newton's Method (50 points)\n",
    "\n",
    "In this part of the homework you are expected to analyse 3 optimization methods (1) Vanilla Gradient Descent (2) Gradient Descent with Momentum and (3) Newton's method. Please only fill the specified areas and do not change other parts of the code in order to prevent any mistakes during evaluation.\n",
    "\n",
    "## Part 1.1: Vanilla Gradient Descent vs. Gradient Descent with Momentum (25 points)\n",
    "\n",
    "In Vanilla Gradient Descent (the standard gradient descent algorithm), the parameters are updated in the direction of negative gradient. In equation form: \n",
    "### <center> $ \\textbf{w}^k =\\textbf{w}^{k-1} - \\alpha \\nabla g(\\textbf{w}^{k-1}) $ <center> \n",
    "\n",
    "However, this update rule causes oscillating, or \"zig-zagging\" in the direction of the gradient. In order to overcome this, one can use the exponential average of the past descent directions, which is called \"momentum\". This allows the gradients to change directions more smoothly and eliminates zig-zags, producing better results in optimization. The update rule in equation form:\n",
    "\n",
    "### <center>$ \\textbf{m}^{k-1} = \\beta \\textbf{m}^{k-2} + (\\nabla g(\\textbf{w}^{k-1})) $ <center>\n",
    "\n",
    "### <center> $ \\textbf{w}^k = \\textbf{w}^{k-1} -  \\textbf{m}^{k-1} \\alpha$ <center>\n",
    "\n",
    "### For more information regarding gradient descent with momentum,  <a href=\"https://distill.pub/2017/momentum/\">go to this link </a>\n",
    "\n",
    "\n",
    "\n",
    "Note that when \"beta\" is equal to 0, above update rule is the same as the one in Vanilla Gradient Descent.\n",
    "\n",
    "### Part 1.1.1: Code Up Momentum Accelerated Gradient Descent (20 points)\n",
    "\n",
    "In the below cell, code up the \"momentum accelerated gradient descent\" to find the \"w\" value that minimizes the quadratic function of the form: \n",
    "### <center> $ g(\\textbf{w}) = \\begin{bmatrix} 0.4 & 9 \\end{bmatrix} \\textbf{w}^2 + 0.25 \\textbf{w}^T \\textbf{w}+ sum(\\textbf{w}) $ <center>\n",
    "\n",
    "### Steps:\n",
    "- Make three runs of the algorithm: one run of \"Vanilla Gradient Descent\" i.e make beta = 0, and two runs of \"Momentum Accelerated Gradient Descent with two \"beta\" parameters between 0 and 1 of your choice. Try to choose beta values that gives different results to compare them later on.\n",
    "- You need to define the initial w value randomly. The size of w vector is 2 x 1.\n",
    "- Choose a proper learning rate / step size and use the same learning rate for each run.\n",
    "- Maximum iteration number should be 30.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "static_plotter = optlib.static_plotter.Visualizer();\n",
    "anime_plotter = optlib.animation_plotter.Visualizer();\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentWithMomentum(g, grad_w, w_0=None, lr = None, beta = None, num_iter=30):\n",
    "    '''\n",
    "        g: cost function\n",
    "        grad_w: derivative of g function wrt w\n",
    "        lr: learning rate\n",
    "        beta: momentum parameter [0,1]\n",
    "        num_iter = number of iterations in gradient descent\n",
    "    '''\n",
    "    #Initialize variables\n",
    "    \n",
    "    m = 0 #Momentum term\n",
    "    w_history = [] #List of w's in each iteration, used to plot\n",
    "\n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "    \n",
    "    # (Don't forget to add w_0 to w_history list!)\n",
    "    w = w_0\n",
    "    i = 0\n",
    "    while i < num_iter:\n",
    "        m = beta*m + grad_w(w)\n",
    "        w =w - m*lr\n",
    "        w_history.append(w)\n",
    "        i += 1\n",
    "    #Then apply gradient descent with momentum steps as many time as num_iter\n",
    "        \n",
    "    #Add new w to list\n",
    "    \n",
    "\n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE #####\n",
    "        \n",
    "    return w, w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-f000bae9c012>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-f000bae9c012>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    grad_w(x):\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##### YOUR CODE STARTS HERE ###\n",
    "\n",
    "def g(x):\n",
    "    return np.matmul(np.array([0.4, 9]),np.matmul(x,x)) + 0.25*np.matmul(x.transpose(),x) + np.sum(a) #cost function\n",
    "def grad_w(x):\n",
    "    return 0# gradient function\n",
    "lr = ... \n",
    "betas = [..., ..., ...] #define your beta values \n",
    "w_0 = ... #initialize w with size 2 x 1\n",
    "\n",
    "for beta in betas:\n",
    "    \n",
    "    w, w_history = gradientDescentWithMomentum(g, grad_w, w_0, lr, beta)\n",
    "    \n",
    "    print(\"Gradient Descent w/ Momentum, Beta =\", ...)\n",
    "    print(\"Optimum weight vector found as: \", ...)\n",
    "    print(\"Minimum cost is found as: \", ...\n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE ###\n",
    "    for i in range(len(w_history)):\n",
    "        w_history[i] = np.squeeze(w_history[i])\n",
    "\n",
    "    # show run in both three-dimensions and just the input space via the contour plot\n",
    "    static_plotter.two_input_surface_contour_plot(g, w_history,num_contours = 30,view = [10,40], xmin=-2, xmax=2, ymin=-1, ymax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1.2: Compare and interpret the results. (5 points)\n",
    "**What is the difference between gradient descent with momentum and Vanilla Gradient Descent?\n",
    "What is the effect of the beta parameter?\n",
    "Discuss your results comparing these methods in terms of optimization process, speed and minimum cost results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Newton's Method (25 points)\n",
    "\n",
    "\n",
    "Newton's method finds the local minima by moving towards to stationary points of second order Taylor series approximation, with the following update formula:\n",
    "### <center> $\\text{w}^k = \\text{w}^{k-1} - (\\nabla^2 g(\\text{w}^{k-1}))^{-1} \\nabla g(\\text{w}^{k-1})$ <center>\n",
    "\n",
    "However, as the dimension of $\\text{w}$ increases linearly, computation of the Hessian matrix $\\nabla^2 g(\\text{w}^{k-1})$ and necessary amount of storage increases quadratically at best. The easiest ways to overcome this issue is to sub-sample the Hessian matrix, in which only some part of the Hessian is used in computation. One way of doing this is to only use the diagonal entries of the Hessian matrix. This way, instead of computing the $NxN$ Hessian matrix, only $N$ entries needed to be computed. Update formula is also simplified to a component-wise update: \n",
    "\n",
    "### <center> $ w_n^k = w_n^{k-1} - \\frac{\\frac{\\partial}{\\partial w_n}g(w^{k-1})}{\\frac{\\partial^2}{\\partial w_n^2}g(w^{k-1})} $ <center>\n",
    "\n",
    "for $n=1,2,3...N$.\n",
    "\n",
    "### Part 1.2.1: Code up the sub-sampled Newton's Method (25 points)\n",
    "\n",
    "Code up the \"sub-sampled Newton's Method\" in the relevant notebook cell by ignoring all off-diagonal elements of the Hessian, as explained above, to find the \"$\\text{w}$ that minimizes the same quadratic function of the form:\n",
    "### <center> $ g(\\textbf{w}) = 0.26(\\textbf{w}^T\\textbf{w}) + 0.48(sum(\\textbf{w}))-cos(\\textbf{w}^T\\textbf{w})$ <center>\n",
    "\n",
    "- Define the given function and gradients as necessary and optmize it using 2 methods (1) Newton's method and (2) Gradient descent with momentum. You can use the gradient descent function above and you are supposed to complete the Newton's method function below.\n",
    "- You need to define the initial w value randomly. The size of w vector is 2 x 1.\n",
    "- Choose a proper learning rate / step size for gradient descent function.\n",
    "- Maximum iteration number should be 10 for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewtonsMethodwithSubSampling(g, grad_1st, grad_2nd, num_iter=10, w_0 = None):\n",
    "    '''\n",
    "        num_iter = number of iterations for Newton's Method\n",
    "        w_0: initial w\n",
    "    '''\n",
    "    #Initialize variables\n",
    "    w_history = [] #List of w's in each iteration, used to plot\n",
    "    epsilon = 1e-6 # Add it to denominator to avoid division by zero\n",
    "    \n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "    \n",
    "    # Don't forget to add w_0 into history\n",
    "    \n",
    "    # Implement the update rule for Newton's method\n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE #####\n",
    "    \n",
    "    return w, w_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE STARTS HERE #####\n",
    "g = ...\n",
    "grad_w_1st = ...\n",
    "grad_w_2nd = ...\n",
    "lr = ...\n",
    "w_0 = ... #initialize w \n",
    "   \n",
    "# Call gradient descent and Newton's method functions \n",
    "\n",
    "w_grad, w_history_grad = ...\n",
    "w_newton, w_history_newton = ...\n",
    "\n",
    "print(\"Gradient Descent w/ Momentum\")\n",
    "print(\"Optimum weight vector: \", ...)\n",
    "print(\"Minimum cost: \", ...)\n",
    "\n",
    "\n",
    "for i in range(len(w_history_grad)):\n",
    "    w_history_grad[i] = np.squeeze(w_history_grad[i])\n",
    "    w_history_newton[i] = np.squeeze(w_history_newton[i])\n",
    "\n",
    "# show run in both three-dimensions and just the input space via the contour plot\n",
    "static_plotter.two_input_surface_contour_plot(g, w_history_grad,num_contours = 30,view = [10,40], xmin=-2, xmax=2, ymin=-1, ymax=1)\n",
    "\n",
    "print(\"Newton's Method\")\n",
    "print(\"Optimum weight vector: \", ...)\n",
    "print(\"Minimum cost: \", ...)\n",
    "\n",
    "static_plotter.two_input_surface_contour_plot(g, w_history_newton,num_contours = 30,view = [10,40], xmin=-2, xmax=2, ymin=-1, ymax=1)\n",
    "\n",
    "##### YOUR CODE ENDS HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2.2: Compare and interpret the performances of both algorithms. (5 points)\n",
    "__Which one of the approaches performed better (in terms of convergence speed and minimum result)? What do you think made the difference?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Least Squares Linear Regression (25 points)\n",
    "\n",
    "### Background\n",
    "\n",
    "Using the Least Squares cost, the aim in Linear Regression is to find the $b, \\mathbf{w}$ values that minimize the following cost function:\n",
    "\n",
    "$$\n",
    "g(b, \\mathbf{w})=\\sum_{p=1}^{P}\\left(b+\\mathbf{x}_{p}^{T} \\mathbf{w}-y_{p}\\right)^{2}\n",
    "$$\n",
    "\n",
    "where $P$ is the number of data and $\\mathbf{x}_{p}$ is the $p_{th}$ data in the dataset. Given the more compact notation: \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{x}}_{p}=\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\mathbf{x}_{p}\n",
    "\\end{array}\\right] \\quad \\tilde{\\mathbf{w}}=\\left[\\begin{array}{l}\n",
    "b \\\\\n",
    "\\mathbf{w}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "gradient for the $b, \\mathbf{w}$, or $\\tilde{\\mathbf{w}}$, can be calculated as follows(check blink videos for detailed derivation):\n",
    "\n",
    "$$\n",
    "\\nabla g(\\tilde{\\mathbf{w}})=2 \\sum_{p=1}^{P} \\tilde{\\mathbf{x}}_{p}\\left(\\tilde{\\mathbf{x}}_{p}^{T} \\tilde{\\mathbf{w}}-y_{p}\\right)=2\\left(\\sum_{p=1}^{P} \\tilde{\\mathbf{x}}_{p} \\tilde{\\mathbf{x}}_{p}^{T}\\right) \\tilde{\\mathbf{w}}-2 \\sum_{p=1}^{P} \\tilde{\\mathbf{x}}_{p} y_{p}\n",
    "$$\n",
    "\n",
    "Using the gradient, it is possible to perform gradient descent to minimize the Least squares cost. However, we can also solve the first order system directly to find the global minimum, withouth any iterative process by setting the gradient above to zero.\n",
    "\n",
    "$$\n",
    "\\left(\\sum_{p=1}^{P} \\tilde{\\mathbf{x}}_{p} \\tilde{\\mathbf{x}}_{p}^{T}\\right) \\tilde{\\mathbf{w}}=\\sum_{p=1}^{P} \\tilde{\\mathbf{x}}_{p} y_{p}\n",
    "$$\n",
    "\n",
    "Solving these equations with the assumption $\\sum_{p=1}^{P} \\tilde{\\mathbf{x}}_{p} \\tilde{\\mathbf{x}}_{p}^{T}$ is invertible, we can calculate $\\tilde{\\mathbf{w}}$ as: \n",
    "$$\n",
    "\\tilde{\\mathbf{w}}^{\\star}=\\left(\\sum_{p=1}^{P} \\tilde{\\mathbf{x}}_{p} \\tilde{\\mathbf{x}}_{p}^{T}\\right)^{-1} \\sum_{p=1}^{P} \\tilde{\\mathbf{x}}_{p} y_{p}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## To Do: Solve the associated Linear Regression Least Squares problem\n",
    "\n",
    "Given the U.S. student loan debt data in the file \"student_debt_data.csv\", fit a linear model by solving the linear system of equations for the Least Squares regression fit and find the associated $w$ vector (Don't code up gradient descent). Calculate what the total student debt will be in 2050 according to the linear model you fit. \n",
    "\n",
    "Some of the functionalities are given to you below, such as plotting and loading data. You are also given the template function that you will fill in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is associated with the book\n",
    "# \"Machine Learning Refined\", Cambridge University Press, 2016.\n",
    "# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "data = np.asarray(pd.read_csv('student_debt_data.csv',header = None))\n",
    "x = data[:,0]\n",
    "x.shape = (len(x),1)\n",
    "y = data[:,1]\n",
    "y.shape = (len(y),1)\n",
    "\n",
    "#Plot data\n",
    "plt.scatter(x,y,linewidth = 2)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('debt in trillions of dollars')\n",
    "plt.show()\n",
    "\n",
    "# pad input with ones\n",
    "o = np.ones((len(x),1))\n",
    "x_new = np.concatenate((o,x),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup the linear system associated to minimizing the Least Squares cost function for this problem and solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TO DO: YOUR CODE GOES HERE #####\n",
    "# solve linear system of equations for regression fit\n",
    "def linear_regression_solve_equation(X,y):\n",
    "    pass\n",
    "\n",
    "w = linear_regression_solve_equation(x_new, y)        # weights learned by solving linear system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our line fit to the data we can now predict total student debt in 2050."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out predicted amount of student debt in 2050\n",
    "debt_in_2050 = w[0] + w[1]*2050\n",
    "print ('if this linear trend continues there will be ' + str(debt_in_2050[0]) + ' trillion dollars in student debt in 2050!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets print out the dataset and linear fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Gradient Descent for Logistic Regression (25 points)\n",
    "\n",
    "### Background\n",
    "\n",
    "In logistic regression, the aim is to minimize the \"Least Squares cost\" function, which is:\n",
    "\n",
    "$g(b, w)=\\sum_{p=1}^{P}\\left(\\sigma\\left(b+\\mathbf{x}_{p}^{T} \\mathbf{w}\\right)-y_{p}\\right)^{2}$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, $\\mathbf{x}_{p}=\\left[\\begin{array}{llll}x_{1, p} & x_{2, p} & \\dots & x_{N, p}\\end{array}\\right]^{T}$ and $\\mathbf{w}_{p}=\\left[\\begin{array}{llll}w_{1} & w_{2} & \\dots & w_{N}\\end{array}\\right]^{T}$.\n",
    "\n",
    "The gradient descent step with respect to above Least Squares cost function is then:\n",
    "\n",
    "$\\Delta g(\\tilde{w})=2 \\sum_{p=1}^{P}\\left(\\sigma\\left(\\tilde{x}_{p}^{T} \\tilde{w}\\right)-y_{p}\\right) \\sigma\\left(\\tilde{x}_{p}^{T} \\tilde{w}\\right)\\left(1-\\sigma\\left(\\tilde{x}_{p}^{T} \\tilde{w}\\right)\\right) \\tilde{x}_{p}$\n",
    "\n",
    "where $$\n",
    "\\tilde{\\mathbf{x}}_{p}=\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\mathbf{x}_{p}\n",
    "\\end{array}\\right] \\quad \\tilde{\\mathbf{w}}=\\left[\\begin{array}{c}\n",
    "b \\\\\n",
    "\\mathbf{w}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "This step can be written more compactly by denoting\n",
    "\n",
    "$\\begin{aligned} \\sigma_{p}^{k-1} &=\\sigma\\left(\\tilde{\\mathbf{x}}_{p}^{T} \\tilde{\\mathbf{w}}^{k-1}\\right) \\\\ r_{p}^{k-1} &=2\\left(\\sigma_{p}^{k-1}-y_{p}\\right) \\sigma_{p}^{k-1}\\left(1-\\sigma_{p}^{k-1}\\right) \\end{aligned}$\n",
    "\n",
    "for all $p=1, \\ldots, P$ and $\\mathbf{r}^{k-1}=\\left[\\begin{array}{llll}r_{1}^{k-1} & r_{2}^{k-1} & \\ldots & r_{P}^{k-1}\\end{array}\\right]^{T}$, and stacking the $\\tilde{\\mathbf{x}}_{p}$ column-wise into the matrix $\\tilde{\\mathbf{X}}$. Then the gradient can be written as:\n",
    "\n",
    "$\\Delta g\\left(\\tilde{\\mathbf{w}}^{k-1}\\right)=\\tilde{\\mathbf{X}} \\mathbf{r}^{k-1}$\n",
    "\n",
    "For programming languages like Python and MATLAB/OCTAVE that have especially efficient implementations of matrix/vector operations this can be much more efficient than explicitly summing over the P points. In other words, these steps are shown for programming simplicity on Python/Matlab like languages.\n",
    "\n",
    "### Complete gradient descent function\n",
    "\n",
    "In this exercise you will produce the gradient descent paths for the dataset \"bacteria_data.csv\". An example figure for the same process on a __\"different\"__ version of the dataset shown in the Figure below:\n",
    "\n",
    "<img src=\"https://web.itu.edu.tr/kamard/Fig3_1_note.png\" style=\"width: 600px;\" />\n",
    "\n",
    "The surface in this figure was generated via the wrapper below with the dataset $bacteria\\_data.csv$, and inside the wrapper you must complete a short gradient descent function to produce the descent paths called\n",
    "\n",
    "$[i n, o u t]=g r a d_{-} d e s c e n t\\left(\\tilde{\\mathbf{X}}, \\mathbf{y}, \\tilde{\\mathbf{w}}^{0}\\right)$\n",
    "\n",
    "where \"in\" and \"out\" contain the gradient steps $\\tilde{\\mathbf{w}}^{k}=\\tilde{\\mathbf{w}}^{k-1}-\\alpha_{k} \\nabla g\\left(\\tilde{\\mathbf{w}}^{k-1}\\right)$ taken corresponding objective value $g\\left(\\tilde{\\mathbf{w}}^{k}\\right)$ respectively, $\\tilde{\\mathbf{X}}$ is the input data matrix, $y$ the output values and $\\tilde{\\mathbf{w}}^{0}$ the initial point.\n",
    "\n",
    "Almost all of this function has already been constructed for you. For example, the step length is fixed at $\\alpha_{k}=10^{-2}$ for all iterations, etc., and you must only enter the gradient of the associated cost function. Pressing \"run\" in the editor will run gradient descent and will produce your figure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "def load_data(csvname):\n",
    "    data = np.asarray(pd.read_csv(csvname,header = None))\n",
    "    x = data[:,0]\n",
    "    x.shape = (np.size(x),1)\n",
    "    temp = np.ones((np.size(x),1))\n",
    "    X = np.concatenate((temp,x),1)\n",
    "    y = data[:,1]\n",
    "    y = y/y.max()\n",
    "    y.shape = (np.size(y),1)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality required for a proper gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TO DO: YOUR CODE GOES HERE - COMPLETE THE GRADIENT DESCENT CODE #####\n",
    "# run gradient descent\n",
    "def gradient_descent(X,y,w0):\n",
    "    w_path = []                 # container for weights learned at each iteration\n",
    "    cost_path = []              # container for associated objective values at each iteration\n",
    "    w_path.append(w0)\n",
    "    cost = compute_cost(w0)\n",
    "    cost_path.append(cost)\n",
    "    w = w0\n",
    "\n",
    "    # start gradient descent loop\n",
    "    max_its = 5000\n",
    "    alpha = 10**(-2)\n",
    "    for k in range(max_its):\n",
    "        # compute gradient\n",
    "        \n",
    "        # take gradient step\n",
    "        w = w - alpha*grad\n",
    "\n",
    "        # update path containers\n",
    "        w_path.append(w)\n",
    "        cost = compute_cost(w)\n",
    "        cost_path.append(cost)\n",
    "\n",
    "    # reshape containers for use in plotting in 3d\n",
    "    w_path = np.asarray(w_path)\n",
    "    w_path.shape = (np.shape(w_path)[0],np.shape(w_path)[1])\n",
    "    return w_path,cost_path\n",
    "\n",
    "# calculate the cost value for a given input weight w\n",
    "def compute_cost(w):\n",
    "    temp = 1/(1 + my_exp(-np.dot(X,w))) - y\n",
    "    temp = np.dot(temp.T,temp)\n",
    "    return temp[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting functions necessary to producing the data, fit, cost surface, and descent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used by plot_logistic_surface to make objective surface of logistic regression cost function\n",
    "def add_layer(a,b,c):\n",
    "    a.shape = (2,1)\n",
    "    b.shape = (1,1)\n",
    "    z = my_exp(-np.dot(c,a))\n",
    "    z = 1/(1 + z) - b\n",
    "    z = z**2\n",
    "    return z\n",
    "\n",
    "# plot fit to data and corresponding gradient descent path onto the logistic regression objective surface\n",
    "def show_fit(w_path,ax,col):\n",
    "    # plot solution of gradient descent fit to original data\n",
    "    s = np.linspace(0,25,100)\n",
    "    t = 1/(1 + my_exp(-(w_path[-1][0] + w_path[-1][1]*s)))\n",
    "    ax.plot(s,t,color = col)\n",
    "\n",
    "# plot gradient descent paths on cost surface\n",
    "def show_paths(w_path,cost_path,ax,col):           \n",
    "    # plot grad descent path onto surface\n",
    "    ax.plot(w_path[:,0],w_path[:,1],cost_path,color = col,linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n",
    "    \n",
    "# plot logistic regression surface\n",
    "def plot_surface(ax):\n",
    "    # plot logistic regression surface\n",
    "    r = np.linspace(-3,3,100)\n",
    "    s,t = np.meshgrid(r, r)\n",
    "    s = np.reshape(s,(np.size(s),1))\n",
    "    t = np.reshape(t,(np.size(t),1))\n",
    "    h = np.concatenate((s,t),1)\n",
    "\n",
    "    # build 3d surface\n",
    "    surf = np.zeros((np.size(s),1))\n",
    "    max_its = np.size(y)\n",
    "    for i in range(0,max_its):\n",
    "        surf = surf + add_layer(X[i,:],y[i],h)\n",
    "\n",
    "    # reshape \n",
    "    s = np.reshape(s,(100,100))\n",
    "    t = np.reshape(t,(100,100))\n",
    "    surf = np.reshape(surf,(100,100))\n",
    "\n",
    "    # plot 3d surface\n",
    "    ax.plot_surface(s,t,surf,cmap = 'jet')\n",
    "    ax.azim = 175\n",
    "    ax.elev = 20\n",
    "    \n",
    "# plot points\n",
    "def plot_points(X,y,ax):\n",
    "    ax.plot(X[:,1],y,'ko')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is defined we can run all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X,y = load_data('bacteria_data.csv') # load in data\n",
    "\n",
    "# initialize figure, plot data, and dress up panels with axes labels etc.,\n",
    "fig = plt.figure(facecolor = 'white',figsize = (8,3))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xlim(min(X[:,1])-0.5, max(X[:,1])+0.5)\n",
    "ax1.set_ylim(min(y)-0.2,max(y)+0.1)\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.xaxis.set_rotate_label(False)\n",
    "ax2.yaxis.set_rotate_label(False)\n",
    "ax2.zaxis.set_rotate_label(False)\n",
    "ax2.get_xaxis().set_ticks([-3,-1,1,3])\n",
    "ax2.get_yaxis().set_ticks([-3,-1,1,3])\n",
    "# ax2.axis('off')\n",
    "\n",
    "### run gradient descent with first initial point\n",
    "w0 = np.array([0,2])\n",
    "w0.shape = (2,1)\n",
    "w_path, cost_path = gradient_descent(X,y,w0)\n",
    "\n",
    "# plot points\n",
    "plot_points(X,y,ax1)\n",
    "\n",
    "# plot fit to data and path on objective surface\n",
    "show_fit(w_path,ax1,'m')\n",
    "show_paths(w_path,cost_path,ax2,'m')\n",
    "\n",
    "### run gradient descent with first initial point\n",
    "w0 = np.array([0,-2])\n",
    "w0.shape = (2,1)\n",
    "w_path, cost_path = gradient_descent(X,y,w0)\n",
    "\n",
    "# plot fit to data and path on objective surface\n",
    "show_fit(w_path,ax1,'c')\n",
    "show_paths(w_path,cost_path,ax2,'c')\n",
    "plot_surface(ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
